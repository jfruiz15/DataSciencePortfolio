---
title: "London Final Project DSC 520"
author: "Jennifer Ruiz"
date: "May 27, 2020"
output:
  word_document: default
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
library(tidyr)
library(ggplot2)
library(dplyr)
library(readr)
library(gtools)
library(tidyverse)
library(lubridate)
```

```{r}
london <- read.csv("london.csv", stringsAsFactors = FALSE)
```

```{r}
glimpse(london)
```
```{r}
summary(london)
```
```{r}
str(london)
```


```{r}
Times <- format(as.POSIXct(strptime(london$timestamp,"%m/%d/%Y %H:%M",tz="")) ,format = "%H:%M")
Dates <- format(as.POSIXct(strptime(london$timestamp,"%m/%d/%Y %H:%M",tz="")) ,format = "%m/%d/%Y")
## had to separate out date and time to properly work with the variable and combine datasets

london$time <- Times
london$date <- Dates
```

```{r}
head(london)
```

```{r}
london %>% 
  rename(
    actual_temp = t1,
    feels_like = t2
    )
```

```{r}
# removing timestamp column from dataset after separating out date and time information
london$timestamp <- NULL
```

```{r}
london$weather_code <- as.factor(london$weather_code)
london$is_holiday <- as.factor(london$is_holiday)
london$is_weekend <- as.factor(london$is_weekend)
london$season <- as.factor(london$season)
```

```{r}
str(london)
```


## Starting Data Visualizations
```{r}
# visualizing number of bike shares by time
ggplot(london, aes(x=time, y =cnt, color = time)) + geom_line() + geom_jitter() +xlab("Time of Day") +ylab("Number of Bike Shares") + ggtitle("Bike Shares based on Time of Day")
```

```{r}

 Season_info <- c("Spring", "Summer", "Fall", "Winter")

# visualizing number of bike shares by seaon
ggplot(london, aes(x=season, y = cnt, color = season)) +geom_point() + geom_jitter() + xlab("Season") + ylab("Number of Bike Shares") + ggtitle("Bike Shares based on season") + scale_color_discrete(name = "Season", labels = c("Spring", "Summer", "Fall", "Winter")) + scale_x_discrete(labels= Season_info)
```

```{r}
#Visualizing bike shares based on season
ggplot(london, aes(x=t1, y = cnt, color=t1)) +geom_point() + geom_jitter() + xlab("Temperature") + ylab("Number of Bike Shares") + ggtitle("Bike Shares based on Temperature in Celsius")
```

```{r}
#visualizing weekend vs weekday data
ggplot(london, aes(x=is_weekend, fill= is_weekend)) + geom_bar() +labs(y= "Bike Share count") + ggtitle("Weekend Bike Shares") + scale_fill_discrete(name = "Day of Bike Share", labels = c("Weekday", "Weekend"))
```

```{r}
#Examining the probability of when rides occur
prop.table(table(london$is_weekend))
```
## Probability table indicates that 71% of ride shares occur during Weekdays and only 29% occur on the weekends.

```{r}
#visualizing holiday data
ggplot(london, aes(x=is_holiday, fill= is_holiday)) + geom_bar() +labs(y= "Bike Share count") + ggtitle("Holiday Bike Shares") + scale_fill_discrete(name = "Holiday Bike Share", labels = c("Non-Holiday", "Holiday"))
```
```{r}
 weather_info <- c("Clear", "Scattered Clouds", "Broken Clouds", "Cloudy", "Rain", "Thunderstorm", "Snowfall", "Freezing Fog")

# visualizing number of bike shares by seaon
ggplot(london, aes(x=weather_code, y = cnt, color = weather_code)) +geom_point() + geom_jitter() + xlab("Weather") + ylab("Number of Bike Shares") + ggtitle("Bike Shares based on weather") + scale_color_discrete(name = "Weather", labels = weather_info) + scale_x_discrete(labels= weather_info)
```

```{r}
#Examining the probability of bike shares on holidays
prop.table(table(london$is_holiday))
```


```{r}
# Looking at Correlations within the data
london_num = london[,c(1:5)] 

res <- cor(london_num)
round(res, 2)
res
```

```{r}
#install.packages("Hmisc")
library(Hmisc)
```

```{r}
flattenCorrMatrix <- function(cormat, pmat) {
  ut <- upper.tri(cormat)
  data.frame(
    row = rownames(cormat)[row(cormat)[ut]],
    column = rownames(cormat)[col(cormat)[ut]],
    cor  =(cormat)[ut],
    p = pmat[ut]
    )
}

res2<-rcorr(as.matrix(res))
flattenCorrMatrix(res2$r, res2$P)

```


```{r}
library(corrplot)

corrplot(res, type = "upper", order = "hclust", 
         tl.col = "black", tl.srt = 45, method = "square")
```

```{r}
# Histogram for cnt (number of bike shares)

ggplot(london, aes(x=cnt)) + geom_histogram() + labs(x= "Number of bike shares") + ggtitle("Histogram of Bike Shares")
```

```{r}
## creating models

mod <- lm(cnt ~ t1, data = london)

```

```{r}
summary(mod)
```

```{r}
#finding the correlation coefficient of the model
sqrt(0.15)
```
## The correlation coefficient of the model is 0.39. This indicates a mild, positive correlation between the variables.

```{r}
# building second model

mod2<- lm(cnt ~ season + weather_code + wind_speed, data = london)
```

```{r}
summary(mod2)
```

```{r}
#finding the correlation coefficient
sqrt(0.11)
```
## The correlation coefficient is 0.33, which indicates a mild, positive correlation between the variables.

```{r}
#nstall.packages("lm.beta")
#nstall.packages("lmtest")
```


```{r}
## calculating the standardized betas for the multiple regression mode

library(lm.beta)
lm.beta(mod2)
```

```{r}
# calculating the confidence intervals
confint(mod2)
```

```{r}
# performing anova to compare models
anova(mod, mod2)
```
## This Model 2 does not significantly improve the model fit compared to Model 1. 

```{r}
# Creating 3rd model
mod3 <- lm(cnt ~ t1 + time + is_weekend, data = london)
```

```{r}
summary(mod3)
```
```{r}
#calculating correlation coefficient of model 3
sqrt(0.68)
```
## The correlation coefficient is 0.82 indicating a strong positive correlation.

```{r}
lm.beta(mod3)
```

```{r}
confint(mod3)
```
## These variables are good predictors as they have tight confidence intervals that do not cross zero. This also indicates that they are representative of the values of the population.

```{r}
library(car)
library(ROCR)
library(dplyr)
library(class)
library(caret)
library(caTools)

```


```{r}
#create random number that is 90% of the total number of rows 
ran <- sample(1:nrow(london), 0.9 * nrow(london))
#creating the normalization function
nor <- function(x) { (x-min(x))/ (max(x)-min(x))}
london_norm <- as.data.frame(lapply(london[,c(1,2,3)], nor))
summary(london_norm)
#training set
london_train <- london_norm[ran,]

#test set
london_test <- london_norm[-ran,]
#extract 1st column of train and test datasets because it will be used as "cl" argument
london_target_category <- london[ran, 1]

london_test_category <- london[-ran,1]
# used the formula k = sqrt(n)/2 to find k value. Used training dataset n for value
pr <- knn(london_train, london_test, cl=london_target_category, k=6)
#create the confusion matrix
tab <- table(pr,london_test_category)
accuracy <- function (x) {sum(diag(x)/ (sum(rowSums(x)))) * 100}
accuracy(tab)


```

## End of Project!!!